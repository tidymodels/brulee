% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lantern_linear_reg-fit.R
\name{lantern_linear_reg}
\alias{lantern_linear_reg}
\alias{lantern_linear_reg.default}
\alias{lantern_linear_reg.data.frame}
\alias{lantern_linear_reg.matrix}
\alias{lantern_linear_reg.formula}
\alias{lantern_linear_reg.recipe}
\title{Fit a linear regression model}
\usage{
lantern_linear_reg(x, ...)

\method{lantern_linear_reg}{default}(x, ...)

\method{lantern_linear_reg}{data.frame}(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

\method{lantern_linear_reg}{matrix}(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

\method{lantern_linear_reg}{formula}(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

\method{lantern_linear_reg}{recipe}(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  validation = 0.1,
  optimizer = "LBFGS",
  learn_rate = 1,
  momentum = 0,
  batch_size = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{x}{Depending on the context:

  * A __data frame__ of predictors.
  * A __matrix__ of predictors.
  * A __recipe__ specifying a set of preprocessing steps
    created from [recipes::recipe()].

 The predictor data should be standardized (e.g. centered or scaled).}

\item{...}{Not currently used, but required for extensibility.}

\item{y}{When `x` is a __data frame__ or __matrix__, `y` is the outcome
specified as:

  * A __data frame__ with 1 numeric column.
  * A __matrix__ with 1 numeric column.
  * A numeric __vector__.}

\item{epochs}{An integer for the number of epochs of training.}

\item{penalty}{The amount of weight decay (i.e., L2 regularization).}

\item{validation}{The proportion of the data randomly assigned to a
validation set.}

\item{optimizer}{The method used in the optimization procedure. Possible choices
are 'LBFGS' and 'SGD'. Default is 'LBFGS'.}

\item{learn_rate}{A positive number. Default is 1 for LBFGS; smaller values
are normally chosen for other optimizers. (`optimizer = "SGD"` only)}

\item{momentum}{A positive number on `[0, 1]` for the momentum parameter in
gradient descent. (`optimizer = "SGD"` only)}

\item{batch_size}{An integer for the number of training set points in each
batch.}

\item{stop_iter}{A non-negative integer for how many iterations with no
improvement before stopping.}

\item{verbose}{A logical that prints out the iteration history.}

\item{formula}{A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.}

\item{data}{When a __recipe__ or __formula__ is used, `data` is specified as:

  * A __data frame__ containing both the predictors and the outcome.}
}
\value{
A `lantern_linear_reg` object with elements:
 * `models`: a list object of serialized models for each epoch before stopping.
 * `best_epoch`: an integer for the epoch with the smallest loss.
 * `loss`: A vector of loss values (MSE) at each epoch.
 * `dim`: A list of data dimensions.
 * `y_stats`: A list of summary statistics for numeric outcomes.
 * `parameters`: A list of some tuning parameter values.
 * `blueprint`: The `hardhat` blueprint data.
}
\description{
`lantern_linear_reg()` fits a model.
}
\details{
The _predictors_ data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric.

The function internally standardizes the
outcome data to have mean zero and a standard deviation of one. The prediction
function creates predictions on the original scale.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {

 ## -----------------------------------------------------------------------------

 data(ames, package = "modeldata")

 ames$Sale_Price <- log10(ames$Sale_Price)

 set.seed(122)
 in_train <- sample(1:nrow(ames), 2000)
 ames_train <- ames[ in_train,]
 ames_test  <- ames[-in_train,]


 # Using matrices
 set.seed(1)
 lantern_linear_reg(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
                    y = ames_train$Sale_Price,
                    penalty = 0.10, epochs = 1, batch_size = 64)

 # Using recipe
 library(recipes)

 ames_rec <-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) \%>\%
    # Transform some highly skewed predictors
    step_BoxCox(Lot_Area, Gr_Liv_Area) \%>\%
    # Lump some rarely occuring categories into "other"
    step_other(Neighborhood, threshold = 0.05)  \%>\%
    # Encode categorical predictors as binary.
    step_dummy(all_nominal(), one_hot = TRUE) \%>\%
    # Add an interaction effect:
    step_interact(~ starts_with("Central_Air"):Year_Built) \%>\%
    step_zv(all_predictors()) \%>\%
    step_normalize(all_predictors())

 set.seed(2)
 fit <- lantern_linear_reg(ames_rec, data = ames_train,
                           epochs = 5, batch_size = 32)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) \%>\%
   bind_cols(ames_test) \%>\%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) \%>\%
   bind_cols(ames_test) \%>\%
   rmse(Sale_Price, .pred)

 }

}
}
