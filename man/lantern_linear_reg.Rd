% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lantern_linear_reg-fit.R
\name{lantern_linear_reg}
\alias{lantern_linear_reg}
\alias{lantern_linear_reg.default}
\alias{lantern_linear_reg.data.frame}
\alias{lantern_linear_reg.matrix}
\alias{lantern_linear_reg.formula}
\alias{lantern_linear_reg.recipe}
\title{Fit a linear regression model}
\usage{
lantern_linear_reg(x, ...)

\method{lantern_linear_reg}{default}(x, ...)

\method{lantern_linear_reg}{data.frame}(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  validation = 0,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)

\method{lantern_linear_reg}{matrix}(
  x,
  y,
  epochs = 20L,
  penalty = 0.001,
  validation = 0,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)

\method{lantern_linear_reg}{formula}(
  formula,
  data,
  epochs = 20L,
  penalty = 0.001,
  validation = 0,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)

\method{lantern_linear_reg}{recipe}(
  x,
  data,
  epochs = 20L,
  penalty = 0.001,
  validation = 0,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{x}{Depending on the context:
\itemize{
\item A \strong{data frame} of predictors.
\item A \strong{matrix} of predictors.
\item A \strong{recipe} specifying a set of preprocessing steps
created from \code{\link[recipes:recipe]{recipes::recipe()}}.
}

The predictor data should be standardized (e.g. centered or scaled).}

\item{...}{Not currently used, but required for extensibility.}

\item{y}{When \code{x} is a \strong{data frame} or \strong{matrix}, \code{y} is the outcome
specified as:
\itemize{
\item A \strong{data frame} with 1 numeric column.
\item A \strong{matrix} with 1 numeric column.
\item A numeric \strong{vector}.
}}

\item{epochs}{An integer for the number of epochs of training.}

\item{penalty}{The amount of weight decay (i.e., L2 regularization).}

\item{validation}{The proportion of the data randomly assigned to a
validation set.}

\item{learn_rate}{A positive number (usually less than 0.1).}

\item{momentum}{A positive number on \verb{[0, 1]} for the momentum parameter in
gradient decent.}

\item{batch_size}{An integer for the number of training set points in each
batch.}

\item{conv_crit}{A non-negative number for convergence.}

\item{verbose}{A logical that prints out the iteration history.}

\item{formula}{A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.}

\item{data}{When a \strong{recipe} or \strong{formula} is used, \code{data} is specified as:
\itemize{
\item A \strong{data frame} containing both the predictors and the outcome.
}}
}
\value{
A \code{lantern_linear_reg} object with elements:
\itemize{
\item \code{models}: a list object of serialized models for each epoch.
\item \code{loss}: A vector of loss values (MSE) at each epoch.
\item \code{dim}: A list of data dimensions.
\item \code{y_stats}: A list of summary statistics for numeric outcomes.
\item \code{parameters}: A list of some tuning parameter values.
\item \code{blueprint}: The \code{hardhat} blueprint data.
}
}
\description{
\code{lantern_linear_reg()} fits a model.
}
\details{
The \emph{predictors} data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric.

The function internally standardizes the
outcome data to have mean zero and a standard deviation of one. The prediction
function creates predictions on the original scale.

If \code{conv_crit} is used, it stops training when the difference in the loss
function is below \code{conv_crit} or if it gets worse. The default trains the
model over the specified number of epochs.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {

 ## -----------------------------------------------------------------------------

 data(ames, package = "modeldata")

 ames$Sale_Price <- log10(ames$Sale_Price)

 set.seed(122)
 in_train <- sample(1:nrow(ames), 2000)
 ames_train <- ames[ in_train,]
 ames_test  <- ames[-in_train,]


 # Using matrices
 set.seed(1)
 lantern_linear_reg(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
                    y = ames_train$Sale_Price,
                    penalty = 0.10, epochs = 20, batch_size = 32)

 # Using recipe
 library(recipes)

 ames_rec <-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) \%>\%
    # Transform some highly skewed predictors
    step_BoxCox(Lot_Area, Gr_Liv_Area) \%>\%
    # Lump some rarely occuring categories into "other"
    step_other(Neighborhood, threshold = 0.05)  \%>\%
    # Encode categorical predictors as binary.
    step_dummy(all_nominal(), one_hot = TRUE) \%>\%
    # Add an interaction effect:
    step_interact(~ starts_with("Central_Air"):Year_Built) \%>\%
    step_zv(all_predictors()) \%>\%
    step_normalize(all_predictors())

 set.seed(2)
 fit <- lantern_linear_reg(ames_rec, data = ames_train,
                           epochs = 20, batch_size = 32)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) \%>\%
   bind_cols(ames_test) \%>\%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) \%>\%
   bind_cols(ames_test) \%>\%
   rmse(Sale_Price, .pred)

 }

}
}
