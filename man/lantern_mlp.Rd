% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/torch_mlp-fit.R
\name{lantern_mlp}
\alias{lantern_mlp}
\alias{lantern_mlp.default}
\alias{lantern_mlp.data.frame}
\alias{lantern_mlp.matrix}
\alias{lantern_mlp.formula}
\alias{lantern_mlp.recipe}
\title{Fit a single layer neural network using torch}
\usage{
lantern_mlp(x, ...)

\method{lantern_mlp}{default}(x, ...)

\method{lantern_mlp}{data.frame}(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0,
  dropout = 0,
  validation = 0.1,
  learning_rate = 0.01,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)

\method{lantern_mlp}{matrix}(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0,
  dropout = 0,
  validation = 0.1,
  learning_rate = 0.01,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)

\method{lantern_mlp}{formula}(
  formula,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0,
  dropout = 0,
  validation = 0.1,
  learning_rate = 0.01,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)

\method{lantern_mlp}{recipe}(
  x,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0,
  dropout = 0,
  validation = 0.1,
  learning_rate = 0.01,
  batch_size = NULL,
  conv_crit = -Inf,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{x}{Depending on the context:
\itemize{
\item A \strong{data frame} of predictors.
\item A \strong{matrix} of predictors.
\item A \strong{recipe} specifying a set of preprocessing steps
created from \code{\link[recipes:recipe]{recipes::recipe()}}.
}

The predictor data should be standardized (e.g. centered or scaled).}

\item{...}{Not currently used, but required for extensibility.}

\item{y}{When \code{x} is a \strong{data frame} or \strong{matrix}, \code{y} is the outcome
specified as:
\itemize{
\item A \strong{data frame} with 1 numeric column.
\item A \strong{matrix} with 1 numeric column.
\item A numeric \strong{vector}.
}}

\item{epochs}{An integer for the number of epochs of training.}

\item{hidden_units}{An integer for the number of hidden units.}

\item{activation}{A string for the activation function. Possible values are
"relu", and "elu".}

\item{penalty}{The amount of weight decay (i.e., L2 regularization).}

\item{dropout}{The proportion of parameters set to zero.}

\item{validation}{The proportion of the data randomly assigned to a
validation set.}

\item{learning_rate}{A positive number (usually less than 0.1).}

\item{batch_size}{An integer for the number of training set points in each
batch.}

\item{conv_crit}{A non-negative number for convergence.}

\item{verbose}{A logical that prints out the iteration history.}

\item{formula}{A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.}

\item{data}{When a \strong{recipe} or \strong{formula} is used, \code{data} is specified as:
\itemize{
\item A \strong{data frame} containing both the predictors and the outcome.
}}
}
\value{
A \code{lantern_mlp} object with elements:
\itemize{
\item \code{models}: a list object of serialized models for each epoch.
\item \code{loss}: A vector of loss values (MSE for regression, negative log-
likelihood for classification) at each epoch.
\item \code{dim}: A list of data dimensions.
\item \code{parameters}: A list of some tuning parameter values.
\item \code{blueprint}: The \code{hardhat} blueprint data.
}
}
\description{
\code{lantern_mlp()} fits a model.
}
\details{
This function fits single layer, feed-forward neural network models for
regression (when the outcome is a number) or classification (a factor). For
regression, the mean squared error is optimized and cross-entropy is the loss
function for classification.

The predictors data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric.

If \code{conv_crit} is used, it stops training when the difference in the loss
function is below \code{conv_crit} or if it gets worse. The default trains the
model over the specified number of epochs.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {

 ## -----------------------------------------------------------------------------
 # regression examples (increase # epochs to get better results)

 data(ames, package = "modeldata")

 ames$Sale_Price <- log10(ames$Sale_Price)

 set.seed(122)
 in_train <- sample(1:nrow(ames), 2000)
 ames_train <- ames[ in_train,]
 ames_test  <- ames[-in_train,]


 # Using matrices
 set.seed(1)
 lantern_mlp(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
           y = ames_train$Sale_Price,
           penalty = 0.10, epochs = 20, batch_size = 32)

 # Using recipe
 library(recipes)

 ames_rec <-
  recipe(Sale_Price ~ Longitude + Latitude + Alley, data = ames_train) \%>\%
  step_dummy(Alley) \%>\%
  step_normalize(all_predictors())

 set.seed(1)
 fit <- lantern_mlp(ames_rec, data = ames_train,
                  dropout = 0.25, epochs = 20, batch_size = 32)
 fit

 autoplot(fit)

 predict(fit, ames_test)
 }

}
}
