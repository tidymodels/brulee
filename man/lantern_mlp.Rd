% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lantern_mlp-fit.R
\name{lantern_mlp}
\alias{lantern_mlp}
\alias{lantern_mlp.default}
\alias{lantern_mlp.data.frame}
\alias{lantern_mlp.matrix}
\alias{lantern_mlp.formula}
\alias{lantern_mlp.recipe}
\title{Fit a single layer neural network}
\usage{
lantern_mlp(x, ...)

\method{lantern_mlp}{default}(x, ...)

\method{lantern_mlp}{data.frame}(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  dropout = 0,
  validation = 0.1,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

\method{lantern_mlp}{matrix}(
  x,
  y,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  dropout = 0,
  validation = 0.1,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

\method{lantern_mlp}{formula}(
  formula,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  dropout = 0,
  validation = 0.1,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)

\method{lantern_mlp}{recipe}(
  x,
  data,
  epochs = 100L,
  hidden_units = 3L,
  activation = "relu",
  penalty = 0.001,
  dropout = 0,
  validation = 0.1,
  learn_rate = 0.01,
  momentum = 0,
  batch_size = NULL,
  class_weights = NULL,
  stop_iter = 5,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{x}{Depending on the context:

  * A __data frame__ of predictors.
  * A __matrix__ of predictors.
  * A __recipe__ specifying a set of preprocessing steps
    created from [recipes::recipe()].

 The predictor data should be standardized (e.g. centered or scaled).}

\item{...}{Not currently used, but required for extensibility.}

\item{y}{When `x` is a __data frame__ or __matrix__, `y` is the outcome
specified as:

  * A __data frame__ with 1 numeric column.
  * A __matrix__ with 1 numeric column.
  * A numeric __vector__.}

\item{epochs}{An integer for the number of epochs of training.}

\item{hidden_units}{An integer for the number of hidden units, or a vector
of integers. If a vector of integers, the model will have `length(hidden_units)`
layers each with `hidden_units[i]` hidden units.}

\item{activation}{A string for the activation function. Possible values are
"relu", "elu", "tanh", and "linear". If `hidden_units` is a vector, `activation`
can be a character vector with length equals to `length(hidden_units)` specifying
the activation for each hidden layer.}

\item{penalty}{The amount of weight decay (i.e., L2 regularization).}

\item{dropout}{The proportion of parameters set to zero.}

\item{validation}{The proportion of the data randomly assigned to a
validation set.}

\item{learn_rate}{A positive number. Default is 1 for LBFGS; smaller values
are normally chosen for other optimizers. (`optimizer = "SGD"` only)}

\item{momentum}{A positive number on `[0, 1]` for the momentum parameter in
gradient descent. (`optimizer = "SGD"` only)}

\item{batch_size}{An integer for the number of training set points in each
batch.}

\item{class_weights}{Numeric class weights (classification only). The value
can be:

 * A named numeric vector (in any order) where the names are the outcome
   factor levels.
 * An unnamed numeric vector assumed to be in the same order as the outcome
   factor levels.
 * A single numeric value for the least frequent class in the training data
   and all other classes receive a weight of one.}

\item{stop_iter}{A non-negative integer for how many iterations with no
improvement before stopping.}

\item{verbose}{A logical that prints out the iteration history.}

\item{formula}{A formula specifying the outcome terms on the left-hand side,
and the predictor terms on the right-hand side.}

\item{data}{When a __recipe__ or __formula__ is used, `data` is specified as:

  * A __data frame__ containing both the predictors and the outcome.}
}
\value{
A `lantern_mlp` object with elements:
 * `models`: a list object of serialized models for each epoch before stopping.
 * `best_epoch`: an integer for the epoch with the smallest loss.
 * `loss`: A vector of loss values (MSE for regression, negative log-
           likelihood for classification) at each epoch.
 * `dim`: A list of data dimensions.
 * `y_stats`: A list of summary statistics for numeric outcomes.
 * `parameters`: A list of some tuning parameter values.
 * `blueprint`: The `hardhat` blueprint data.
}
\description{
`lantern_mlp()` fits a model.
}
\details{
This function fits single layer, feed-forward neural network models for
regression (when the outcome is a number) or classification (a factor). For
regression, the mean squared error is optimized and cross-entropy is the loss
function for classification.

The _predictors_ data should all be numeric and encoded in the same units (e.g.
standardized to the same range or distribution). If there are factor
predictors, use a recipe or formula to create indicator variables (or some
other method) to make them numeric.

When the outcome is a number, the function internally standardizes the
outcome data to have mean zero and a standard deviation of one. The prediction
function creates predictions on the original scale.
}
\examples{
\donttest{
if (torch::torch_is_installed()) {

 ## -----------------------------------------------------------------------------
 # regression examples (increase # epochs to get better results)

 data(ames, package = "modeldata")

 ames$Sale_Price <- log10(ames$Sale_Price)

 set.seed(122)
 in_train <- sample(1:nrow(ames), 2000)
 ames_train <- ames[ in_train,]
 ames_test  <- ames[-in_train,]


 # Using matrices
 set.seed(1)
 fit <-
   lantern_mlp(x = as.matrix(ames_train[, c("Longitude", "Latitude")]),
               y = ames_train$Sale_Price,
               penalty = 0.10, batch_size = 2^8)

 # Using recipe
 library(recipes)

 ames_rec <-
  recipe(Sale_Price ~ Bldg_Type + Neighborhood + Year_Built + Gr_Liv_Area +
         Full_Bath + Year_Sold + Lot_Area + Central_Air + Longitude + Latitude,
         data = ames_train) \%>\%
   # Transform some highly skewed predictors
   step_BoxCox(Lot_Area, Gr_Liv_Area) \%>\%
   # Lump some rarely occuring categories into "other"
   step_other(Neighborhood, threshold = 0.05)  \%>\%
   # Encode categorical predictors as binary.
   step_dummy(all_nominal(), one_hot = TRUE) \%>\%
   # Add an interaction effect:
   step_interact(~ starts_with("Central_Air"):Year_Built) \%>\%
   step_zv(all_predictors()) \%>\%
   step_normalize(all_predictors())

 set.seed(2)
 fit <- lantern_mlp(ames_rec, data = ames_train, hidden_units = 20,
                    dropout = 0.05, batch_size = 2^8)
 fit

 autoplot(fit)

 library(ggplot2)

 predict(fit, ames_test) \%>\%
   bind_cols(ames_test) \%>\%
   ggplot(aes(x = .pred, y = Sale_Price)) +
   geom_abline(col = "green") +
   geom_point(alpha = .3) +
   lims(x = c(4, 6), y = c(4, 6)) +
   coord_fixed(ratio = 1)

 library(yardstick)
 predict(fit, ames_test) \%>\%
   bind_cols(ames_test) \%>\%
   rmse(Sale_Price, .pred)
 }

 # ------------------------------------------------------------------------------
 # classification

 library(dplyr)
 library(ggplot2)

 data("parabolic", package = "modeldata")

 set.seed(1)
 in_train <- sample(1:nrow(parabolic), 300)
 parabolic_tr <- parabolic[ in_train,]
 parabolic_te <- parabolic[-in_train,]

 set.seed(2)
 cls_fit <- lantern_mlp(class ~ ., data = parabolic_tr, hidden_units = 2,
                        epochs = 200L, learn_rate = 0.1, activation = "elu",
                        penalty = 0.1, batch_size = 2^8)
 autoplot(cls_fit)

 grid_points <- seq(-4, 4, length.out = 100)

 grid <- expand.grid(X1 = grid_points, X2 = grid_points)

 predict(cls_fit, grid, type = "prob") \%>\%
  bind_cols(grid) \%>\%
  ggplot(aes(X1, X2)) +
  geom_contour(aes(z = .pred_Class1), breaks = 1/2, col = "black") +
  geom_point(data = parabolic_te, aes(col = class))

}
}
