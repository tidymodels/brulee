% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mlp_torch.R
\name{details_mlp_torch}
\alias{details_mlp_torch}
\title{Multilayer perceptron via torch}
\description{
\code{\link[=lantern_mlp]{lantern_mlp()}} fits a feed-forward neural network to numeric or
categorical outcomes.
}
\details{
For this engine, there are multiple modes: classification and regression
\subsection{Tuning Parameters}{

This model has 5 tuning parameters listed as a main argument:
\itemize{
\item \code{hidden_units}: # Hidden Units (type: integer, default: 5L)
\item \code{penalty}: Amount of Regularization (type: double, default: 0.001)
\item \code{epochs}: # Epochs (type: integer, default: 100L)
\item \code{dropout}: Dropout Rate (type: double, default: 0.0)
\item \code{activation}: Activation Function (type: character, default: ‘relu’)
}

For \code{penalty}, the amount of regularization is \emph{only} L2 penalty (i.e.,
ridge or weight decay).

There are other parameters that can be set or tuned via
\code{\link[parsnip:set_engine]{parsnip::set_engine()}}:
\itemize{
\item \code{momentum}: The amount to adjust the gradient direction/length with
the value from a previous iteration. (default: 0.0,
\code{optimizer = 'SGD'} only)
\item \code{batch_size}: (default: )
\item \code{class_weights}: Numeric class weights (classification only). The
value for this argument could be:
\itemize{
\item A named numeric vector (in any order) where the names are the
outcome factor levels.
\item An unnamed numeric vector assumed to be in the same order as the
outcome factor levels.
\item A single numeric value for the least frequent class in the
training data and all other classes receive a weight of one.
}
}

Other relevant arguments that are not tuning parameters:
\itemize{
\item \code{validation}: The proportion of the data randomly assigned to a
validation set. (default: 0.1)
\item \code{optimizer}: The method used in the optimization procedure. Possible
choices are \code{'LBFGS'} and \code{'SGD'}. (default: \code{'LBFGS'})
}
}

\subsection{Translation from parsnip to the original package (regression)}{\if{html}{\out{<div class="sourceCode r">}}\preformatted{mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  activation = character(1)
) \%>\%  
  set_engine("torch") \%>\% 
  set_mode("regression") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Single Layer Neural Network Specification (regression)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
## 
## Computational engine: torch 
## 
## Model fit template:
## lantern::lantern_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     decay = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1))
}
}

\subsection{Translation from parsnip to the original package (classification)}{\if{html}{\out{<div class="sourceCode r">}}\preformatted{mlp(
  hidden_units = integer(1),
  penalty = double(1),
  dropout = double(1),
  epochs = integer(1),
  activation = character(1)
) \%>\% 
  set_engine("torch") \%>\% 
  set_mode("classification") \%>\% 
  translate()
}\if{html}{\out{</div>}}\preformatted{## Single Layer Neural Network Specification (classification)
## 
## Main Arguments:
##   hidden_units = integer(1)
##   penalty = double(1)
##   dropout = double(1)
##   epochs = integer(1)
##   activation = character(1)
## 
## Computational engine: torch 
## 
## Model fit template:
## lantern::lantern_mlp(x = missing_arg(), y = missing_arg(), hidden_units = integer(1), 
##     decay = double(1), dropout = double(1), epochs = integer(1), 
##     activation = character(1))
}
}

\subsection{Preprocessing requirements}{

Factor/categorical predictors need to be converted to numeric values
(e.g., dummy or indicator variables) for this engine. When using the
formula method via
\code{\link[=fit.model_spec]{fit.model_spec()}}, parsnip will
convert factor columns to indicators.

Predictors should have the same scale. One way to achieve this is to
center and scale each so that each predictor has mean zero and a
variance of one.
}

\subsection{References}{
\itemize{
\item Kuhn, M, and K Johnson. 2013. \emph{Applied Predictive Modeling}.
Springer.
}
}
}
\keyword{internal}
